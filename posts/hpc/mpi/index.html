<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>MPI - </title><meta name=author content="ghy"><meta name=author-link content="https://github.com/gaohongy"><meta name=description content="mpicc, mpic++, mpicxx mpiexec, mpirun mpichversion mpicc 仅可编译 .c 文件，编译 .cpp文件会报错 mpic++ 编译 .cpp 文件 MPI标准的不同实现：MPICH、MVAPICH、MVAPICH2、Open"><meta name=keywords content='Hugo,FixIt'><meta itemprop=name content="MPI"><meta itemprop=description content="mpicc, mpic++, mpicxx mpiexec, mpirun mpichversion mpicc 仅可编译 .c 文件，编译 .cpp文件会报错 mpic++ 编译 .cpp 文件 MPI标准的不同实现：MPICH、MVAPICH、MVAPICH2、Open"><meta itemprop=datePublished content="2023-12-12T21:49:28+08:00"><meta itemprop=dateModified content="2024-06-03T11:55:26+08:00"><meta itemprop=wordCount content="6344"><meta itemprop=keywords content="HPC"><meta property="og:url" content="https://gaohongy.github.io/blog/posts/hpc/mpi/"><meta property="og:title" content="MPI"><meta property="og:description" content="mpicc, mpic++, mpicxx mpiexec, mpirun mpichversion mpicc 仅可编译 .c 文件，编译 .cpp文件会报错 mpic++ 编译 .cpp 文件 MPI标准的不同实现：MPICH、MVAPICH、MVAPICH2、Open"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-12-12T21:49:28+08:00"><meta property="article:modified_time" content="2024-06-03T11:55:26+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="MPI"><meta name=twitter:description content="mpicc, mpic++, mpicxx mpiexec, mpirun mpichversion mpicc 仅可编译 .c 文件，编译 .cpp文件会报错 mpic++ 编译 .cpp 文件 MPI标准的不同实现：MPICH、MVAPICH、MVAPICH2、Open"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical href=https://gaohongy.github.io/blog/posts/hpc/mpi/><link rel=prev href=https://gaohongy.github.io/blog/posts/computer-architecture/computer-architecture/><link rel=next href=https://gaohongy.github.io/blog/posts/algorithm/approximation-algorithm/><link rel=stylesheet href=/blog/css/style.min.css><link rel=stylesheet href=/blog/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/blog/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"MPI","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/gaohongy.github.io\/blog\/posts\/hpc\/mpi\/"},"genre":"posts","wordcount":6344,"url":"https:\/\/gaohongy.github.io\/blog\/posts\/hpc\/mpi\/","datePublished":"2023-12-12T21:49:28+08:00","dateModified":"2024-06-03T11:55:26+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"ghy"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/blog/ title><img loading=lazy src=https://cdn.jsdelivr.net/gh/G-ghy/cloudImages@master/202301210616317.png data-title=https://cdn.jsdelivr.net/gh/G-ghy/cloudImages@master/202301210616317.png data-alt=https://cdn.jsdelivr.net/gh/G-ghy/cloudImages@master/202301210616317.png class=logo style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text></span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/blog/posts/>文章</a></li><li class=menu-item><a class=menu-link href=/blog/categories/>分类</a></li><li class=menu-item><a class=menu-link href=/blog/tags/>标签</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/blog/ title><img loading=lazy src=https://cdn.jsdelivr.net/gh/G-ghy/cloudImages@master/202301210616317.png data-title=https://cdn.jsdelivr.net/gh/G-ghy/cloudImages@master/202301210616317.png data-alt=https://cdn.jsdelivr.net/gh/G-ghy/cloudImages@master/202301210616317.png class=logo style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text></span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/blog/posts/>文章</a></li><li class=menu-item><a class=menu-link href=/blog/categories/>分类</a></li><li class=menu-item><a class=menu-link href=/blog/tags/>标签</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><nav aria-label=breadcrumb class=breadcrumb-container><ol class=breadcrumb><li class=breadcrumb-item><a href=/blog/ title>主页</a></li><li class=breadcrumb-item><a href=/blog/posts/ title=Posts>Posts</a></li><li class="breadcrumb-item active" aria-current=page>MPI</li></ol></nav><main class=container><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>MPI</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/gaohongy title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src=https://cdn.jsdelivr.net/gh/G-ghy/cloudImages@master/202205161528792.jpg data-title=ghy data-alt=ghy class=avatar style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;ghy</a></span>
<span class=post-category>收录于 <a href=/blog/categories/hpc/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> HPC</a></span></div><div class=post-meta-line><span title="发布于 2023-12-12 21:49:28"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2023-12-12>2023-12-12</time></span>&nbsp;<span title="更新于 2024-06-03 11:55:26"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-06-03>2024-06-03</time></span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#mpi-introduce>MPI Introduce</a></li><li><a href=#mpi-install>MPI Install</a></li><li><a href=#hello-world>Hello World</a></li><li><a href=#mpi-environment-management>MPI Environment Management</a></li><li><a href=#naming-style>Naming Style</a></li><li><a href=#datatype>Datatype</a><ul><li><a href=#contiguous>contiguous</a></li><li><a href=#vector>vector</a></li><li><a href=#index>index</a></li><li><a href=#struct>struct</a></li></ul></li><li><a href=#mpi-execution-mode>MPI Execution Mode</a></li><li><a href=#communication>Communication</a><ul><li><a href=#阻塞式>阻塞式</a></li><li><a href=#非阻塞式>非阻塞式</a></li><li><a href=#deadlock>Deadlock</a></li><li><a href=#消息查询>消息查询</a><ul><li><a href=#dynamic-receiving>Dynamic Receiving</a></li></ul></li><li><a href=#communicator>Communicator</a><ul><li><a href=#hign-dimension-communicator>Hign Dimension Communicator</a></li></ul></li><li><a href=#broadcast>Broadcast</a></li><li><a href=#gather>Gather</a></li><li><a href=#scatter>Scatter</a></li><li><a href=#alltoall>AlltoAll</a></li></ul></li><li><a href=#parallel-calculation-mode>Parallel Calculation Mode</a><ul><li><a href=#reduce>Reduce</a></li><li><a href=#scan>Scan</a></li><li><a href=#reduce_scatter>Reduce_scatter</a></li><li><a href=#user-define-operation>user define operation</a></li></ul></li><li><a href=#cuda-aware-mpi>CUDA-Aware MPI</a></li><li><a href=#network>Network</a></li><li><a href=#reference>Reference</a></li></ul></nav></div></div><div class=content id=content data-end-flag=EOF><p>mpicc, mpic++, mpicxx
mpiexec, mpirun
mpichversion</p><p>mpicc 仅可编译 .c 文件，编译 .cpp文件会报错
mpic++ 编译 .cpp 文件</p><p>MPI标准的不同实现：MPICH、MVAPICH、MVAPICH2、Open MPI</p><p>MPI编译器mpicc只是对于普通c/c++编译器的封装，在普通编译器的基础上添加了MPI相关文件的路径便于进行头文件引入和链接</p><p>mpiexec is a direct link to mpiexec.hydra</p><p>mpirun is a wrapper script that determines your batch system and simplifies the launch for the user, but eventually also calls mpiexec.hydra</p><p>通信子 &lt;=> 进程集合</p><p>MPI_Init 会初始化一个通信子communicator MPI_COMM_WORLD</p><h2 id=mpi-introduce>MPI Introduce</h2><p>All parallel in MPI is explicit.</p><p>MPI implementation version:</p><ol><li>MVAPICH</li><li>MPICH</li><li>Inter MPI</li><li>Open MPI</li><li>Microsoft MPI</li></ol><h2 id=mpi-install>MPI Install</h2><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>./configure -prefix<span class=o>=</span>/home/hongyu_gao2001/mpi/mpi-install --disable-fortran
</span></span><span class=line><span class=cl>make
</span></span><span class=line><span class=cl>make install</span></span></code></pre></td></tr></table></div></div><p>If we execute the command <code>man mpicc</code> or <code>man mpicxx</code>, we can get two command line arguments <code>-compile_info</code> and <code>-link_info</code>, which are used to show the steps for compiling a program(what options and include paths are used) and show the steps for linking a program(what options and libraries are used).</p><p>If we use the <code>mpicc -compile_info</code> or <code>mpicc -link_info</code>, we can get the following compile options:</p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>-I/home/hongyu_gao2001/mpi/mpi-install/include 
</span></span><span class=line><span class=cl>-L/home/hongyu_gao2001/mpi/mpi-install/lib 
</span></span><span class=line><span class=cl>-Wl,-rpath 
</span></span><span class=line><span class=cl>-Wl,/home/hongyu_gao2001/mpi/mpi-install/lib 
</span></span><span class=line><span class=cl>-Wl,--enable-new-dtags 
</span></span><span class=line><span class=cl>-lmpi</span></span></code></pre></td></tr></table></div></div><blockquote><p>For a detailed explanation of the above options, please refer to the <a href=https://gaohongy.github.io/blog/posts/compile_link/c-c++-compile-link/#%E7%BC%96%E8%AF%91%E6%B5%81%E7%A8%8B target=_blank rel="external nofollow noopener noreferrer">C C++ Compile Link</a>.</p></blockquote><p>Acording to the result of the option <code>mpicc -compile_info</code> or the <code>mpicc -link_info</code>, we can learn about that the mpicc and mpicxx is just a wrapper of gcc and g++, them add extra options for gcc and g++.</p><p>And <code>-prefix=</code> option and <code>make install</code> make the mpicc and mpicxx carry the compile option according to the content of the <code>-prefix=</code>, which is equal to the gcc or g++ which carry these optinos manually.</p><p>According to the <a href="https://en.wikipedia.org/wiki/Rpath#cite_note-1:~:text=readelf%20%2Dd%20%3Cbinary_name%3E%20%7C%20grep%20%27R.*PATH%27%20displays%20the%20RPATH%20or%20RUNPATH%20of%20a%20binary%20file.%20In%20gcc%2C%20for%20instance%2C%20one%20could%20specify%20RPATH%20by%20%2DWl%2C%2Drpath%2C/custom/rpath/." target=_blank rel="external nofollow noopener noreferrer">rpath | wikipedia</a>, we can use the <code>readelf -d &lt;binary_name> | grep 'R.*PATH'</code> to verify the role of the compile option <code>-Wl</code>.</p><div class=highlight id=id-3><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ gcc mpi_hello_world.c -I/home/hongyu_gao2001/mpi/mpi-install/include -L/home/hongyu_gao2001/mpi/mpi-install/lib -lmpi -o mpi_hello_world_norpath
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>$ ./mpi_hello_world_norpath 
</span></span><span class=line><span class=cl>./mpi_hello_world_norpath: error <span class=k>while</span> loading shared libraries: libmpi.so.12: cannot open shared object file: No such file or directory
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>$ readelf -d mpi_hello_world_norpath  <span class=p>|</span> grep <span class=err>&#39;</span>R.*PATH</span></span></code></pre></td></tr></table></div></div><div class=highlight id=id-4><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ mpicc mpi_hello_world.c -o mpi_hello_world
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>$ ./mpi_hello_world 
</span></span><span class=line><span class=cl>./mpi_hello_world: error <span class=k>while</span> loading shared libraries: libmpi.so.12: cannot open shared object file: No such file or directory
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>$ readelf -d mpi_hello_world  <span class=p>|</span> grep <span class=err>&#39;</span>R.*PATH
</span></span><span class=line><span class=cl>0x000000000000001d <span class=o>(</span>RUNPATH<span class=o>)</span>            Library runpath: <span class=o>[</span>/home/hongyu_gao2001/mpi/mpi-install/lib<span class=o>]</span></span></span></code></pre></td></tr></table></div></div><p>The GNU Linker (GNU ld) implements a feature which it calls &ldquo;new-dtags&rdquo;, which can be used to insert an rpath that has lower precedence than the LD_LIBRARY_PATH environment variable.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><h2 id=hello-world>Hello World</h2><div class=highlight id=id-5><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=nf>MPI_Init</span><span class=p>(</span><span class=nb>NULL</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// Get the number of processes
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=n>world_size</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=nf>MPI_Comm_size</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>world_size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// Get the rank of the process
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=n>world_rank</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=nf>MPI_Comm_rank</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>world_rank</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nf>MPI_Finalize</span><span class=p>();</span></span></span></code></pre></td></tr></table></div></div><h2 id=mpi-environment-management>MPI Environment Management</h2><ol><li><code>int MPI_Init(int *argc, char ***argv)</code></li></ol><p>When we call this function, system will generate a communicator called <code>MPI_COMM_WORLD</code></p><h2 id=naming-style>Naming Style</h2><p>If all the letters are the capital letters, it always be a constant, such as <code>MPI_AINT</code>. It is like the <code>MPI_INT</code>.</p><p>If the letters contains the capital letters and lowercase letters, it always be a type, such as <code>MPI_Aint</code>.</p><h2 id=datatype>Datatype</h2><h3 id=contiguous>contiguous</h3><p>用于连续的数据</p><h3 id=vector>vector</h3><p>数据块的大小 和 两个数据块之间的距离 均为常数</p><p><a class=lightgallery href="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405261210050.png?size=large" data-thumbnail="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405261210050.png?size=small" data-sub-html="<h2>https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405261210050.png</h2>"><img loading=lazy src=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405261210050.png srcset="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405261210050.png?size=small, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405261210050.png?size=medium 1.5x, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405261210050.png?size=large 2x" sizes=auto data-title=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405261210050.png data-alt=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405261210050.png style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></a></p><p>Please note that vector is not only used for one dimension scene，it can also used for two dimension matrix.</p><p>One or two dimension is just a concept of programming language. For MPI, no matter what types of matrix, it all sees as the one dimension in memory, e.g. it only cares about the structure of memory.</p><p>So, if we want to describe a submatrix, we can also use the <code>MPI_Type_vector()</code>, just edit the input matrix structure.</p><p>In my opinion, upper bound and lower bound are just memory addresses,</p><h3 id=index>index</h3><p>与 vector 类似，但数据块大小和两个数据块之间的距离均不是常数，用于补充 vector 的应用场景</p><p>MPI 发送和接收数据并不是要求派生数据类型必须相同，而是要保证发送和接收数据之间能够匹配上，发送时使用派生数据类型，接收时是可以使用原生数据类型的，参考<a href=https://rookiehpc.org/mpi/docs/mpi_type_indexed/index.html target=_blank rel="external nofollow noopener noreferrer">MPI_Type_indexed</a>。</p><h3 id=struct>struct</h3><p>最灵活的一种，灵活性来源于可配置项的增加。In fact, the most important configuration items are <code>block_lengths[]</code>, <code>displacements[]</code> and <code>block_types[]</code>.</p><ul><li>vector: these three configuration items are immobilized.</li><li>index: can specify the <code>block_length</code> and <code>displacement</code> respectivly.</li><li>struct: can specify these three configuration items respectivly.</li></ul><p>Please note that, the number of <code>block_length</code> corresponds to the <code>block_types</code>.</p><p>Although the name of this type is struct, we not only treat it as a true <code>C struct</code>, but also mix of any other data types.</p><h2 id=mpi-execution-mode>MPI Execution Mode</h2><p>If we create some arrays in main function and execute the MPI program with many processes. These arrays in different processes are private datas.</p><h2 id=communication>Communication</h2><p>MPI use the Point-to-Point(P2P, 点对点通信), 显式采用双边通信，需要进程双方的参与，在 MPI 2.0 中提出了单边通信的概念。</p><p>P2P 的两种具体实现协议：(所谓实现协议，是指这部分内容并不属于 MPI Standard)</p><ol><li>Eager</li><li>Rendezvous</li></ol><h3 id=阻塞式>阻塞式</h3><p>MPI 提供了一些数据结构，对应C/C++中的基础数据结构，用于在更高层次指定消息结构</p><p>Receiver 如果收不到消息会一直阻塞等待。</p><p>Sender 是否会阻塞取决于 MPI 的具体实现，某些实现下，在没有对应的 Receiver 时，Sender 会阻塞；在某些实现下，Sender会将数据存储到 send buffer 中，然后直接返回，并不会阻塞。这一点区别会影响到对于 MPI deadlock 情况的分析。</p><div class=highlight id=id-6><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=n>world_rank</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=nf>MPI_Comm_rank</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>world_rank</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>world_size</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=nf>MPI_Comm_size</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>world_size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>number</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>world_rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>number</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>number</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=k>if</span> <span class=p>(</span><span class=n>world_rank</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>number</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=n>MPI_STATUS_IGNORE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Process 1 received number %d from process 0</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>number</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>Please note that, the <code>count</code> field in <code>MPI_Send</code> means the number of data which will be send, but it means the max capacity of receive buffer.</p><p>我们称 <code>MPI_Send</code> 和 <code>MPI_Recv</code> 是阻塞式的，这一说法来源于<a href=https://www.mpi-forum.org/docs/ target=_blank rel="external nofollow noopener noreferrer">MPI Document</a>，然后它们的 actual performance is maybe different from what we understand.</p><p>Before we talk about some situations which can explain this statement, we need to discriminate some concepts:</p><ol><li><p>MPI is a standard, it contains many rules to tell us what should MPI do and how it do.</p></li><li><p>MPICH and OpenMPI are implementation of MPI</p></li></ol><p>In <a href=https://www.mpi-forum.org/docs/ target=_blank rel="external nofollow noopener noreferrer">MPI standard</a>, <code>MPI_Send</code> is a blocking function.</p><p>But in <a href=https://www.mpich.org/static/docs/v4.0.x/www3/MPI_Send.html target=_blank rel="external nofollow noopener noreferrer">MPICH MPI_Send doc</a>, we can see the following statement:</p><blockquote><p>This routine <strong>may</strong> block until the message is received by the destination process.</p></blockquote><p><strong>How can we understand this &ldquo;may&rdquo; ?</strong></p><p>Answer: <strong>In MPICH</strong>, when the <code>MPI_Send</code> can&rsquo;t find the enough send buffer, it will block. or else, it will return.</p><p>There is an example <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> to elaborate this viewpoint</p><p>If we follow the rule of MPI standard, <code>MPI_Send</code> will block entil <code>MPI_Recv</code> is executed. But the following example doesn&rsquo;t follow this rules</p><div class=highlight id=id-7><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;iostream&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;mpi.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Init</span><span class=p>(</span><span class=nb>NULL</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>size</span><span class=p>,</span> <span class=n>rank</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Comm_size</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Comm_rank</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>rank</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>buffer</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>msg</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 0: &#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>msg</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=n>MPI_STATUS_IGNORE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>msg</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>buffer</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>msg</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 1: &#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>msg</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=n>MPI_STATUS_IGNORE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>msg</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Finalize</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>The output of program is:</p><blockquote><p>process 0: 1</p><p>process 1: 0</p></blockquote><p>According to the rule of MPI standard, this program can&rsquo;t execute, but it get the right result now.</p><p>There is a precondition of this program can get the right result is that <code>MPI_Send</code> can return normally before the corresponding <code>MPI_Recv</code> function completes execution.</p><p>So, we can use the following program to verify this guess.</p><div class=highlight id=id-8><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;iostream&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;mpi.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;unistd.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Init</span><span class=p>(</span><span class=nb>NULL</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>size</span><span class=p>,</span> <span class=n>rank</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Comm_size</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Comm_rank</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>rank</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>buffer1</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>buffer2</span> <span class=o>=</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;MPI_Send returns&#34;</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>msg</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>sleep</span><span class=p>(</span><span class=mi>10</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>msg</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=n>MPI_STATUS_IGNORE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 1 receives the msg: &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>msg</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>msg</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>sleep</span><span class=p>(</span><span class=mi>5</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>msg</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=n>MPI_STATUS_IGNORE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 2 receives the msg: &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>msg</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Finalize</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>This program will get the following result:</p><blockquote><p>MPI_Send returns</p><p>process 2 receives the msg: 2</p><p>process 1 receives the msg: 1</p></blockquote><p>And we can also use the following extreme example</p><div class=highlight id=id-9><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;iostream&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;mpi.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;unistd.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Init</span><span class=p>(</span><span class=nb>NULL</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>size</span><span class=p>,</span> <span class=n>rank</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Comm_size</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Comm_rank</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>rank</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>buffer1</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>buffer2</span> <span class=o>=</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;MPI_Send returns&#34;</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Finalize</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>This program can also execute rightly, and will get the following result</p><blockquote><p>MPI_Send returns</p></blockquote><p>Now, let&rsquo;s review which problem do we face, we have a guess that &ldquo;<code>MPI_Send</code> can return normally before the corresponding <code>MPI_Recv</code> function completes execution&rdquo;. Now we use these two examples to verify this guess. So we can also explain the fist problem that the program doesn&rsquo;t follow the MPI standard but it can also finish execution.</p><p>In fact, in MPICH, there is explain of execution principle of <code>MPI_Send</code></p><blockquote><p>The message sent by each MPI process has to be copied out before the send operation completes and the receive operation starts.</p></blockquote><p>In a word, what we want to say is that the detail expression of some functions is decided by the implementation of MPI standard, but when we use these funcitons, we still should follow the rule of MPI standard to prevent some strange results.</p><h3 id=非阻塞式>非阻塞式</h3><p>Because non-blocking function will return immediately after calling. So the data in buffer is maybe not received by receiver process. So, MPI provides the <code>MPI_Request</code>, <code>MPI_Wait()</code> and <code>MPI_Test()</code> to address this problem.</p><h3 id=deadlock>Deadlock</h3><h3 id=消息查询>消息查询</h3><h4 id=dynamic-receiving>Dynamic Receiving</h4><p>利用MPI_Recv的最后一个 MPI_Status 参数来获取接受到的消息的信息，以便应对动态长度的传输数据</p><p>The data structure of MPI_Status:</p><div class=highlight id=id-10><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=k>typedef</span> <span class=k>struct</span> <span class=n>MPI_Status</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>count_lo</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>count_hi_and_cancelled</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>MPI_SOURCE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>MPI_TAG</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>MPI_ERROR</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=n>MPI_Status</span><span class=p>;</span></span></span></code></pre></td></tr></table></div></div><p>An example of using status, but MPI_Recv的问题在于只能提前开辟尽可能大的store buffer，在接受完消息后再通过status得知消息长度</p><div class=highlight id=id-11><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=cp>#define MAX_NUMBERS 100
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>numbers</span><span class=p>[</span><span class=n>MAX_NUMBERS</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>world_rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Send</span><span class=p>(</span><span class=n>numbers</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>number_amount</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Status</span> <span class=n>status</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Recv</span><span class=p>(</span><span class=n>numbers</span><span class=p>,</span> <span class=n>MAX_NUMBERS</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Get_count</span><span class=p>(</span><span class=o>&amp;</span><span class=n>status</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>number_amount</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;%d</span><span class=se>\n</span><span class=s>%d</span><span class=se>\n</span><span class=s>%d</span><span class=se>\n</span><span class=s>%d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>number_amount</span><span class=p>,</span> <span class=n>status</span><span class=p>.</span><span class=n>MPI_SOURCE</span><span class=p>,</span> <span class=n>status</span><span class=p>.</span> <span class=n>MPI_TAG</span><span class=p>,</span> <span class=n>status</span><span class=p>.</span><span class=n>MPI_ERROR</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>替代方案是首先使用 MPI_Probe 获取消息长度，然后根据消息长度开辟存储空间，再使用 MPI_Recv 接收消息，避免空间浪费（不过这只是逻辑说法，实际连接起这些概念的都是status，MPI_Probe也是将即将接收到的消息状态存储到status数据结构中）</p><div class=highlight id=id-12><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=cp>#define MAX_NUMBERS 100
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>numbers</span><span class=p>[</span><span class=n>MAX_NUMBERS</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>world_rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Send</span><span class=p>(</span><span class=n>numbers</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>number_count</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Status</span> <span class=n>status</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Probe</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Get_count</span><span class=p>(</span><span class=o>&amp;</span><span class=n>status</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>number_count</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=o>*</span><span class=n>number_buf</span> <span class=o>=</span> <span class=p>(</span><span class=kt>int</span> <span class=o>*</span><span class=p>)</span><span class=nf>malloc</span><span class=p>(</span><span class=k>sizeof</span><span class=p>(</span><span class=kt>int</span><span class=p>)</span> <span class=o>*</span> <span class=n>number_count</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Recv</span><span class=p>(</span><span class=n>number_buf</span><span class=p>,</span> <span class=n>number_count</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=n>MPI_STATUS_IGNORE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;%d</span><span class=se>\n</span><span class=s>%d</span><span class=se>\n</span><span class=s>%d</span><span class=se>\n</span><span class=s>%d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>number_count</span><span class=p>,</span> <span class=n>status</span><span class=p>.</span><span class=n>MPI_SOURCE</span><span class=p>,</span> <span class=n>status</span><span class=p>.</span> <span class=n>MPI_TAG</span><span class=p>,</span> <span class=n>status</span><span class=p>.</span><span class=n>MPI_ERROR</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>free</span><span class=p>(</span><span class=n>number_buf</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><h3 id=communicator>Communicator</h3><h4 id=hign-dimension-communicator>Hign Dimension Communicator</h4><p>这里其实就涉及到我们对于通信子的理解了，如果从计算机网络的角度来说，通信子可以理解为一个广播域（从通信子的使用方式来看，确实也比较符合这种理解方式）</p><p>所谓高维通信子，不过是把进程按照一种新的方式进行分组</p><p>在 MPI 中，我们需要 <code>MPI_Comm_split</code> 实现对现有进程重新分组，我们需要考虑重新分组需要指定哪些内容：</p><ol><li>process 将位于哪个组别</li><li>process 在新的组别中的编号（这里的编号指的就是在 1 维下的编号了）</li></ol><p>以上两项内容分别由函数参数中的 <code>color</code> 和 <code>key</code> 实现</p><p><code>color</code> 相同的被分到同一个组中，也就是同一个通信子中，<code>key</code> 的相对大小关系将决定了 process 在新的通信子内的 rank 排序方式（这里按照目前我的理解，只需要关注 rank 的相对大小，实际编号仍然是从 0 开始，遵循 rank 所指示的相对大小关系）</p><p>实际上我们所说的几维通信子，这里的维度只是人为附加的一种认知信息，所谓认知信息是我们通过分组，为每个进程都添加了额外的分组信息，我们可以通过这些分组信息抽象出高维结构，或者说通过这些分组更好地映射到某些计算任务上，但是这并不代表实际真的存在这样一个高维结构，不过是人为附加的信息罢了。</p><p>需要注意的是，我们一般在划分完高维通讯子后会去获取一下进程在自己的 行通讯子 和 列通讯子中的 rank。</p><div class=highlight id=id-13><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>int</span> <span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>MPI_Comm_rank</span><span class=p>(</span><span class=n>rowcomm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>row</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>MPI_Comm_rank</span><span class=p>(</span><span class=n>colcomm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>col</span><span class=p>);</span></span></span></code></pre></td></tr></table></div></div><p>这里要格外小心，上述代码中的 <code>row</code> 和 <code>col</code> 并不等价于逻辑布局中的行号和列号，以 3 x 2 的布局为例，代码中的 row 和 col 对应到逻辑布局上如下图所示</p><p><a class=lightgallery href="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405302307467.png?size=large" data-thumbnail="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405302307467.png?size=small" data-sub-html="<h2>https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405302307467.png</h2>"><img loading=lazy src=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405302307467.png srcset="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405302307467.png?size=small, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405302307467.png?size=medium 1.5x, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405302307467.png?size=large 2x" sizes=auto data-title=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405302307467.png data-alt=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405302307467.png style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></a></p><p>在上图中，我们用第一个数值表示 process 所在行通讯子中的 rank 值，第二个数值表示 process 所在列通讯子中的 rank 值</p><h3 id=broadcast>Broadcast</h3><p>收发双发都调用 <code>MPI_Bcast</code> 函数，参数也是相同的。(<strong>这一点很重要，因为在实际编程时我们很容易就理解成只需要让发生 broadcast 的通讯子内部的 root node 调用即可，但是实际上需要这一通讯子内部的所有 process 都调用此函数</strong>)</p><p>那么很神奇的就是函数如何判断函数调用者是发送方还是接收方,我觉得这应该是在函数实现中会对调用 process 的 rank 进行判断，无论是发送方还是接收方调用函数，函数参数中的 root 表示的都是发送方，那么函数在实现时只需要判断 root 值是否等于调用者的 rank 即可。</p><h3 id=gather>Gather</h3><p><a class=lightgallery href="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301144067.png?size=large" data-thumbnail="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301144067.png?size=small" data-sub-html="<h2>https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301144067.png</h2>"><img loading=lazy src=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301144067.png srcset="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301144067.png?size=small, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301144067.png?size=medium 1.5x, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301144067.png?size=large 2x" sizes=auto data-title=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301144067.png data-alt=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301144067.png style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></a></p><p><code>MPI_Gather()</code> 一个函数既负责发送，也负责接收，通过参数中的 <code>root</code> 指定 gather 操作的信息接受者</p><p>There are some important things which are described in official document<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><blockquote><ol><li><p>The root receives the messages and stores them in rank order</p></li><li><p>Note that the recvcount argument at the root indicates the number of items it receives from each MPI process, not the total number of items it receives.</p></li></ol></blockquote><p>According to the second item above, <code>MPI_Gather()</code> can only solve the situation that all the processes send the same of data to the root process.</p><p>Thinking about why this function parameter recvcount is defined in this way. In fact, the effect of <code>MPI_Gather</code> is similar with each non root node calls the <code>MPI_Send</code> to send they own data to root node, so this viewpoint can explain the two phenomenons:</p><ol><li>why each process need to call the <code>MPI_Gather</code></li><li>why the meaing of recvcount parameter is the number of items that is received by root node from each process instead of all processes</li></ol><p>If we want to gather data more flexibly, we can use <code>MPI_Gatherv</code>.</p><h3 id=scatter>Scatter</h3><p><a class=lightgallery href="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143628.png?size=large" data-thumbnail="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143628.png?size=small" data-sub-html="<h2>https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143628.png</h2>"><img loading=lazy src=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143628.png srcset="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143628.png?size=small, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143628.png?size=medium 1.5x, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143628.png?size=large 2x" sizes=auto data-title=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143628.png data-alt=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143628.png style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></a></p><p>其实 <code>MPI_Scatter</code> 是 <code>MPI_Gather</code> 的逆过程，但是 scatter 的实际应用过程更容易出现错误。为了便于叙述，我们假定负责进行 gather 和 scatter 的进程均为 0 号进程。在 Gather 过程中，process 0 会汇集其他 process 所传递的数据进行合并，那么 <code>MPI_Gather</code> 将作为一个统一的入口点，在此函数参数中我们无法单独指定每块数据的长度，因此很自然就理解了 MPI_Gather<code>只能处理不同 process 下长度相等的数据块。但是</code>MPI_Scatter` 实际的分发过程是在各个接收 process 中完成的，我们很容易就会理解为让每个 process 接收自定义长度的数据。但是显然这对于 gather 来说，并非严格的逆过程，实际上，scatter 必须在每个 process 中都处理相同长度的数据，即不同 process 中处理的数据长度相等。</p><p>scatter 的 sendcount 指的是发送给一个 process 的数据量，而不是参与 scatter 的全部数据量，那么很容易产生一个疑问，即此时 sendcount 和 recvcount 又有什么区别，既然都是发送给 receiver process 的数据量，为什么不合并为一个 count，还要分为两个参量。产生这个疑问我们可能忽略了 sendtype 和 recvtype 的存在，发送 1 个 4B 的 int，接收 4 个 1B 的 char 是一个合理的需求，此时 sendcount 和 recvcount 并不相等。</p><p>There is an example that can explain this design. Firstly, we need explain some pre-infomations,</p><p>the binary expression of char $A$ is $1000001$, we use the binary expression $1000001010000010100000101000001$ (decimal number $1094795585$) to express the $AAAA$.</p><p>So, if we use four 1 byte char to receive this decimal number, we will get four $A$.</p><p>Now, we use MPI to verify this judgement.</p><div class=highlight id=id-14><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>number</span> <span class=o>=</span> <span class=mi>1094795585</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>char</span> <span class=n>msg</span><span class=p>[</span><span class=mi>4</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Scatter</span><span class=p>(</span><span class=o>&amp;</span><span class=n>number</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>msg</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=n>MPI_CHAR</span><span class=p>,</span> <span class=n>root</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 0: &#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>4</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>msg</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>&lt;&lt;</span> <span class=sc>&#39; &#39;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>The result of the above code is:</p><blockquote><p>process 0: A A A A</p></blockquote><p>不过在这里例子中，我们仅使用到了一个 process，当参与过程的进程不止一个时，这里仍然会让人感到困惑，这是因为我们总是将 <code>MPI_Gather</code> 和 <code>MPI_Scatter</code> 的执行模式看为主从式，即在 gather 中，root 节点负责接收，其他节点负责发送，或者在 scatter 中，root 节点负责发送，其他节点负责接收。然而实际上并非如此，以 root 节点为例，它自己既是发送方又是接收方，to be honest, I don&rsquo;t understand the design idea of scatter, expecially when I need to fill the parameter <code>sendcount</code> of <code>MPI_Scatter</code>, but we can follow a simple rule that $sendcount * sizeof(sendtype) == recvcount * sizeof(recvtype)$。</p><p>Maybe we can understand <code>sendcount</code> and <code>sendtype</code> in this way: when this progress includes multiple processes, we can think that root node need to send the message to other nodes one by one with rank order. So the <code>sendcount</code> and <code>sendtype</code> mean what needs to be sent from root node to an another node.</p><p>Now, we use an example to illustrate the usage of <code>MPI_Scatter</code> for multiple processes:</p><p>Following the above implementation method, we change the data from AAAA to AABB to verify the effect of scatter.</p><p>01000001 01000001 01000010 01000010 (1094795842)</p><pre><code>A       A        B        B
</code></pre><div class=highlight id=id-15><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>number</span> <span class=o>=</span> <span class=mi>1094795842</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=kt>char</span> <span class=n>msg</span><span class=p>[</span><span class=mi>2</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Scatter</span><span class=p>(</span><span class=o>&amp;</span><span class=n>number</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>MPI_CHAR</span><span class=p>,</span> <span class=n>msg</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>MPI_CHAR</span><span class=p>,</span> <span class=n>root</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 0: &#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>2</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>msg</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>&lt;&lt;</span> <span class=sc>&#39; &#39;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=nf>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>char</span> <span class=n>msg</span><span class=p>[</span><span class=mi>2</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=n>MPI_Scatter</span><span class=p>(</span><span class=nb>NULL</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>MPI_CHAR</span><span class=p>,</span> <span class=n>msg</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>MPI_CHAR</span><span class=p>,</span> <span class=n>root</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 1: &#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>2</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>msg</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>&lt;&lt;</span> <span class=sc>&#39; &#39;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>The result of the above code is:</p><blockquote><p>process 0: B B</p><p>process 1: A A</p></blockquote><p>What we need to notice is that both <code>MPI_Gatherv</code> and <code>MPI_Scatterv</code> use the number of element for displacement, they do not use the byte displacement like some creating data type function, such as <code>MPI_Type_create struct</code>.</p><h3 id=alltoall>AlltoAll</h3><p><a class=lightgallery href="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143675.png?size=large" data-thumbnail="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143675.png?size=small" data-sub-html="<h2>https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143675.png</h2>"><img loading=lazy src=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143675.png srcset="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143675.png?size=small, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143675.png?size=medium 1.5x, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143675.png?size=large 2x" sizes=auto data-title=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143675.png data-alt=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301143675.png style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></a></p><p>The principle of <code>MPI_Alltoall</code> is execute multiple times <code>MPI_Send</code> and <code>MPI_Recv</code>, so the parameter <code>sendcount</code> and <code>recvcount</code> in <code>MPI_Alltoall</code> means the message number from sender to receiver instead of total message volume.</p><h2 id=parallel-calculation-mode>Parallel Calculation Mode</h2><h3 id=reduce>Reduce</h3><p><a class=lightgallery href="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301142731.png?size=large" data-thumbnail="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301142731.png?size=small" data-sub-html="<h2>https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301142731.png</h2>"><img loading=lazy src=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301142731.png srcset="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301142731.png?size=small, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301142731.png?size=medium 1.5x, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301142731.png?size=large 2x" sizes=auto data-title=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301142731.png data-alt=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301142731.png style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></a></p><p>对多个元素进行 reduce 时，不同元素 reduce 的结果是分离的，有几个元素进行 reduce 结果就有几个</p><p>值得注意的是，传递给 <code>MPI_Reduce</code> 和 <code>MPI_Allreduce</code> 的 recvbuf 的初始值并不重要，假设进行的 reduce 是求和操作，那么直观感觉上最终的结果应该会累加到 recvbuf 中，但是实际测试结果并非如此，recvbuf 只是承担了存储最终结果的角色，求和并没有在其原始数值上进行</p><div class=highlight id=id-16><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>buffer</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>MPI_Reduce</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>sum</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_SUM</span><span class=p>,</span> <span class=n>root</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>sum</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=nf>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>buffer</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Reduce</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer</span><span class=p>,</span> <span class=k>nullptr</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_SUM</span><span class=p>,</span> <span class=n>root</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=nf>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>buffer</span> <span class=o>=</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Reduce</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer</span><span class=p>,</span> <span class=k>nullptr</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_SUM</span><span class=p>,</span> <span class=n>root</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>在上述程序中，我们并没有对 sum 赋初值 0，但程序依然得到了正确结果 3</p><p><code>MPI_Allreduce</code> 可以理解为两步操作：broadcast + reduce</p><ol><li>Every process broadcasts they onw data to other processes.</li><li>Every process reduce all data and save the result to they own local result buffer.</li></ol><p>Compare with <code>MPI_Reduce</code>，there is only a change in <code>MPI_Allreduce</code> program that <code>MPI_Allreduce</code> don&rsquo;t need the root parameter, because every process is the root node.</p><div class=highlight id=id-17><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>buffer</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>MPI_Allreduce</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>sum</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_SUM</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 0: &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>sum</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=nf>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>buffer</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Allreduce</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>sum</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_SUM</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 1: &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>sum</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=nf>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>buffer</span> <span class=o>=</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Allreduce</span><span class=p>(</span><span class=o>&amp;</span><span class=n>buffer</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>sum</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_SUM</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 2: &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>sum</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>This program will get the following result:</p><blockquote><p>process 0: 3</p><p>process 1: 3</p><p>process 2: 3</p></blockquote><h3 id=scan>Scan</h3><p><a class=lightgallery href="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301141933.png?size=large" data-thumbnail="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301141933.png?size=small" data-sub-html="<h2>https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301141933.png</h2>"><img loading=lazy src=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301141933.png srcset="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301141933.png?size=small, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301141933.png?size=medium 1.5x, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301141933.png?size=large 2x" sizes=auto data-title=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301141933.png data-alt=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301141933.png style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></a></p><p>When judge the multiple channels input data，<code>MPI_Scan</code> 与 <code>MPI_Reduce</code> 的实现方式相同，都是对每个输入通道单独进行 scan 操作，如以下代码所示</p><div class=highlight id=id-18><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>buffer</span><span class=p>[]</span> <span class=o>=</span> <span class=p>{</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>};</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>prefix</span><span class=p>[</span><span class=mi>2</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Scan</span><span class=p>(</span><span class=n>buffer</span><span class=p>,</span> <span class=n>prefix</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_SUM</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 0: &#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>2</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>prefix</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>&lt;&lt;</span> <span class=sc>&#39; &#39;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=nf>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>buffer</span><span class=p>[]</span> <span class=o>=</span> <span class=p>{</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>};</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>prefix</span><span class=p>[</span><span class=mi>2</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Scan</span><span class=p>(</span><span class=n>buffer</span><span class=p>,</span> <span class=n>prefix</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_SUM</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 1: &#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>2</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>prefix</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>&lt;&lt;</span> <span class=sc>&#39; &#39;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=nf>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>buffer</span><span class=p>[]</span> <span class=o>=</span> <span class=p>{</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>};</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>prefix</span><span class=p>[</span><span class=mi>2</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Scan</span><span class=p>(</span><span class=n>buffer</span><span class=p>,</span> <span class=n>prefix</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_SUM</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;process 2: &#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>2</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>prefix</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>&lt;&lt;</span> <span class=sc>&#39; &#39;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>The program will get the following result:</p><blockquote><p>process 0: 1 2</p><p>process 1: 3 5</p><p>process 2: 6 9</p></blockquote><h3 id=reduce_scatter>Reduce_scatter</h3><p><a class=lightgallery href="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301140940.png?size=large" data-thumbnail="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301140940.png?size=small" data-sub-html="<h2>https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301140940.png</h2>"><img loading=lazy src=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301140940.png srcset="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301140940.png?size=small, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301140940.png?size=medium 1.5x, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301140940.png?size=large 2x" sizes=auto data-title=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301140940.png data-alt=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301140940.png style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></a></p><p>The &ldquo;reduce&rdquo; is similar with <code>MPI_Reduce</code>, but the &ldquo;scatter&rdquo; is similar with <code>MPI_Scatterv</code> that it can specify the number of elements for every process.</p><h3 id=user-define-operation>user define operation</h3><p>首先从调用形式上来说，由于 user define operatoin 是直接作用于现有的 <code>MPI_Reduce</code> 和 <code>MPI_Scan</code> 等计算函数的，所以函数接口的形式是可以确定的，不能确定的是 user define operation 的具体代码实现，而这个实现一定是对照实际需求的，如果想要在实际需求和代码实现之间建立起联系，我们必须要考虑的是 <code>MPI_Reduce</code> 以及 <code>MPI_Scan</code> 是以何种方式来调用这个 user define operation 的，只有在了解了执行模型后，我们才能编写出对照实际需求的代码实现。</p><p>在此前分析 <code>MPI_Reduce</code> 和 <code>MPI_Scan</code> 时，我们说法这两个函数在处理多通道数据时，处理逻辑是逐个通道来处理的，这里从定义 operation 时给定的 len parameter 可以看出，实际运算在处理时确实是逐个通道来处理的。</p><p>为了便于阐述，这里以 <code>MPI_Reduce</code> 使用 user define operation 为例</p><p>这里需要区分清几个概念：</p><ol><li>process 调用 <code>MPI_Reduce</code> 时的 sendBuffer 和 recvBuffer</li><li>operation 所面对的 inputBuffer 和 outputBuffer</li></ol><p>inputBuffer 对应非 root node 的 sendBuf，outputBuffer 对应 root node 的 sendBuf。（这里面没有涉及到的是非 root node 和 root node 的recvBuf，对于前者，并不需要提供这个参数，对于后者，只是负责存储最终的结果，中间数据是通过 root node 的 sendBuf 来负责存储的）</p><p>对于 <code>MPI_Reduce</code> 和 <code>MPI_Scan</code> 对于 operation 执行模式的了解有助于帮助我们解决之前所遇到的两个疑问：</p><ol><li><code>MPI_Reduce</code> 并不关心非 root node 的 recvBuf parameter</li><li>root node 在没有初始化 recvBuf 时，最终依然能够得到正确结果（我们可以理解为中间的运算数据都是 root node 的 sendBuf 在存储，最后直接将运算结果存储到 root node 的 recvBuf 中，不过最终结果存储这个过程无法从现有测试中得到验证，只是我们对于实现机制的一种猜测）</li></ol><p>只看上面的内容，依然会对整个过程比较模糊，我们结合下面的图来理解</p><p><a class=lightgallery href="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301842323.png?size=large" data-thumbnail="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301842323.png?size=small" data-sub-html="<h2>https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301842323.png</h2>"><img loading=lazy src=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301842323.png srcset="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301842323.png?size=small, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301842323.png?size=medium 1.5x, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301842323.png?size=large 2x" sizes=auto data-title=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301842323.png data-alt=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202405301842323.png style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></a></p><p>如果有 3 个 process 参与整个过程，那么只有非 root node 会调用 operation，即上图中 call 的两部分，他们会通过蓝色框标识的参数直接与 root node 的数值进行交互，从而获得最终结果，中间运算数据都存储在 root node 的数值中，最终存储到 root node 的 recvBuf 中。</p><h2 id=cuda-aware-mpi>CUDA-Aware MPI</h2><p><a href=https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/ target=_blank rel="external nofollow noopener noreferrer">An Introduction to CUDA-Aware MPI</a></p><h2 id=network>Network</h2><p>RDMA（Remote Direct Memory Access，远程内存直接访问）技术</p><p>让数据能够在不同服务器内存间直接传输， 而不经过 CPU 和操作系统（简单理解就是网卡需要承担 CPU 的部分计算任务）</p><p><a class=lightgallery href="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202406030950957.png?size=large" data-thumbnail="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202406030950957.png?size=small" data-sub-html="<h2>https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202406030950957.png</h2>"><img loading=lazy src=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202406030950957.png srcset="https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202406030950957.png?size=small, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202406030950957.png?size=medium 1.5x, https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202406030950957.png?size=large 2x" sizes=auto data-title=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202406030950957.png data-alt=https://cdn.jsdelivr.net/gh/gaohongy/cloudImages@master/202406030950957.png style="background:url(/blog/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></a></p><p><strong>MPI 和 RDMA 之间有何种联系 ？</strong></p><p>因为 MPI 需要发生多节点之间的通信，而这个数据传输过程如果发生在传统的 TCP/IP 网络中，必然要经过 己方CPU -> 己方网卡NIC -> 对方NIC -> 对方CPU 这样一个过程，这无疑会给 CPU 带来一定计算开销，然而在网络通信中所涉及的计算无非就是一些路由信息的计算，这些工作完全可以交由 NIC 来处理，所以可以通过增强 NIC 的计算能力，让 CPU 从这一通信过程中摘离，即实现 RDMA。</p><p>目前 3 种 RDMA 网络：</p><ul><li>Infiniband</li><li>RoCE</li><li>iWARP</li></ul><h2 id=reference>Reference</h2><blockquote><ul><li>[1] <a href=https://mpitutorial.com/tutorials/ target=_blank rel="external nofollow noopener noreferrer">MPI Tutorial</a></li><li>[2] <a href=https://www.mpich.org/static/docs/latest/ target=_blank rel="external nofollow noopener noreferrer">MPICH API Document</a></li><li>[3] <a href=http://www.xtaohub.com/IT-neo/Parallel-programming-MPI.html target=_blank rel="external nofollow noopener noreferrer">一切靠自己的 MPI 框架</a></li></ul></blockquote><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href="https://en.wikipedia.org/wiki/Rpath#cite_note-1:~:text=The%20GNU%20Linker%20%28GNU%20ld%29%20implements%20a%20feature%20which%20it%20calls%20%22new%2Ddtags%22%2C%20which%20can%20be%20used%20to%20insert%20an%20rpath%20that%20has%20lower%20precedence%20than%20the%20LD_LIBRARY_PATH%20environment%20variable." target=_blank rel="external nofollow noopener noreferrer">The role of GNU ld</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://www.mpi-forum.org/docs/mpi-4.1/mpi41-report.pdf target=_blank rel="external nofollow noopener noreferrer">MPI Standard | 3.5 Semantics of Point-to-Point Communication | Example 3.10</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://www.mpi-forum.org/docs/ target=_blank rel="external nofollow noopener noreferrer">MPI Docs</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-06-03 11:55:26">更新于 2024-06-03&nbsp;</span></div></div><div class=post-info-line><div class=post-info-md><span><a href=https://github.com/gaohongy/blog/edit/main/content/posts/HPC/MPI.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/blog/>主页</a></span></section></div><div class=post-nav><a href=/blog/posts/computer-architecture/computer-architecture/ class=post-nav-item rel=prev title="Computer Architecture"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>Computer Architecture</a>
<a href=/blog/posts/algorithm/approximation-algorithm/ class=post-nav-item rel=next title="Approximation Algorithm">Approximation Algorithm<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/blog/lib/lightgallery/css/lightgallery-bundle.min.css><link rel=stylesheet href=/blog/lib/katex/katex.min.css><link rel=stylesheet href=/blog/lib/cookieconsent/cookieconsent.min.css><script src=/blog/lib/autocomplete/autocomplete.min.js defer></script><script src=/blog/lib/fuse/fuse.min.js defer></script><script src=/blog/lib/twemoji/twemoji.min.js defer></script><script src=/blog/lib/lightgallery/lightgallery.min.js defer></script><script src=/blog/lib/lightgallery/plugins/thumbnail/lg-thumbnail.min.js defer></script><script src=/blog/lib/lightgallery/plugins/zoom/lg-zoom.min.js defer></script><script src=/blog/lib/katex/katex.min.js defer></script><script src=/blog/lib/katex/auto-render.min.js defer></script><script src=/blog/lib/katex/copy-tex.min.js defer></script><script src=/blog/lib/katex/mhchem.min.js defer></script><script src=/blog/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:10},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},lightgallery:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/blog/index.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:20,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:50,threshold:.3,type:"fuse",useExtendedSearch:!1},twemoji:!0}</script><script src=/blog/js/theme.min.js defer></script></body></html>